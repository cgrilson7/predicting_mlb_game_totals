{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Total Runs in MLB Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to provide an end-to-end example of how one can go about gathering, cleaning, modeling, and interpreting sports data. Specifically, we will gather data that allows us to build a classifier that predicts whether or not the total runs scored in MLB games will be above or below the over/under values set by various sportsbooks.\n",
    "\n",
    "For the purposes of this analysis, we will assume that the odds at which a bettor can bet on the over/unders of all games are -110. To demonstrate what this means: suppose a bettor risks 110 dollars that the total runs scored in a particular MLB game will be over 8.5. If 9 or more runs are scored, the bettor wins 100 dollars (and retains his or her initial 110 dollars). If 8 or fewer runs are scored in the game, the bettor loses his or her 110 dollars.\n",
    "\n",
    "Assuming that we can always bet at -110 odds (which is a stable assumption), we need to win our bets 52.4% of the time in order to break even. Thus, our ultimate goal is to build a classifier with accuracy greater than 52.4% of the time (for context, professional sports bettors are estimated to have win rates between 55% and 60%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Obtain Historical Game Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As will be done for all required web scraping in this project, we will use two web scraping libraries, `requests` and `BeautifulSoup`, to access the desired data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import statistics\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chars(a_str, sub):\n",
    "    '''\n",
    "    Auxiliary function to find all instances of a string \"sub\" within a larger string \"a_str\"\n",
    "    '''\n",
    "    start = 0\n",
    "    while True:\n",
    "        start = a_str.find(sub, start)\n",
    "        if start == -1: return\n",
    "        yield start\n",
    "        start += len(sub) # use start += 1 to find overlapping matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html(string, char):\n",
    "    '''\n",
    "    Auxiliary function that returns all characters within a string until character \"char\" is found\n",
    "    '''\n",
    "    output = \"\"\n",
    "    for i in range(len(string)):\n",
    "        if string[i] != char:\n",
    "            output = output + string[i]\n",
    "        else:\n",
    "            break\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pitcher_stats(comment_str, preceding_str, away_pitcher_index, home_pitcher_index):\n",
    "    '''\n",
    "    Auxiliary function that returns specific statistics embedded in a comment, based on certain strings that\n",
    "    precede the statistics of interest.\n",
    "    '''\n",
    "    indexes = [i+len(preceding_str) for i in list(find_chars(comment_str, preceding_str))]\n",
    "    for index in indexes:\n",
    "        if index > away_pitcher_index:\n",
    "            away_pitcher_stat_index = index\n",
    "            break\n",
    "    for index in indexes:\n",
    "        if index > home_pitcher_index:\n",
    "            home_pitcher_stat_index = index\n",
    "            break\n",
    "    away_pitcher_stat = parse_html(comment_str[away_pitcher_stat_index:], \"<\")\n",
    "    home_pitcher_stat = parse_html(comment_str[home_pitcher_stat_index:], \"<\")\n",
    "    return away_pitcher_stat, home_pitcher_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pitching_info(pitching_link, year, month, day):\n",
    "    '''\n",
    "    This function scrapes baseball-reference and retrieves many features related to pitching statistics for\n",
    "    MLB games played on a specific date\n",
    "    \n",
    "    Args:\n",
    "        pitching_link: A web extension that contains detailed pitching information for a given game\n",
    "        year: The year of the game\n",
    "        month: The month of the game\n",
    "        day: The day of the game\n",
    "    \n",
    "    Returns:\n",
    "        away_pitcher: name of the away team starting pitcher\n",
    "        home_pitcher: name of the home team starting pitcher\n",
    "        away_pitcher_IP: away team starting pitcher's innings pitched in game\n",
    "        home_pitcher_IP: home team starting pitcher's innings pitched in game\n",
    "        away_pitcher_postgame_ERA: away team starting pitcher's ERA (including stats from game of interest)\n",
    "        home_pitcher_postgame_ERA: home team starting pitcher's ERA (including stats from game of interest)\n",
    "        away_pitcher_ER: away team starting pitcher's earned runs in game\n",
    "        home_pitcher_ER: home team starting pitcher's earned runs in game\n",
    "        \n",
    "    '''\n",
    "    # define the specific baseball reference link for MLB game pitching statistics\n",
    "    pitcher_url = requests.get(pitching_link)\n",
    "    # ensure page was loaded properly\n",
    "    if pitcher_url.status_code == 200:\n",
    "        # initialize BeautifulSoup object\n",
    "        soup = BeautifulSoup(pitcher_url.content, 'html.parser')\n",
    "        # parse through HTML body to retrieve the element of the site that shows team starting lineups\n",
    "        soup1 = soup.find(\"body\").find(\"div\", {\"id\" : \"wrap\"}).find(\"div\", {\"id\" : \"content\"}) \n",
    "        soup2 = soup1.find(\"div\", {\"class\" : \"grid_wrapper\",\n",
    "                      \"id\" : \"all_lineups\"})\n",
    "        # interestingly, the starting lineups we need to parse are saved as a single HTML comment\n",
    "        # get the comment\n",
    "        comment = soup2.find_all(string=lambda text:isinstance(text,Comment))[0]\n",
    "        comment_str = str(comment) # convert the comment to a string\n",
    "        # each player in a starting lineup is preceded by a string 'html\">'\n",
    "        # while seemingly cryptic, this allows us to quickly identify the indexes where starting pitcher\n",
    "        # names lie within the comment\n",
    "        indexes = [i+6 for i in list(find_chars(comment_str, 'html\">'))]\n",
    "        # depending on whether the game is played at a National League or American league stadium,\n",
    "        # either 9 or 10 players, respectively, will be listed for each team's starting lineup\n",
    "        if len(indexes) == 18: # if National league game, there will be 9 players on each team\n",
    "            away_pitcher_index = indexes[8] # get index corresponding to first char of away pitcher's name\n",
    "            home_pitcher_index = indexes[17] # get index corresponding to first char of home pitcher's name\n",
    "            # retrieve the full names of the two pitchers by calling function parse_html\n",
    "            away_pitcher = parse_html(comment_str[away_pitcher_index:], \"<\")\n",
    "            home_pitcher = parse_html(comment_str[home_pitcher_index:], \"<\")\n",
    "        elif len(indexes) == 20: # if American league game, there will be 10 players on each team\n",
    "            away_pitcher_index = indexes[9] # get index corresponding to first char of away pitcher's name\n",
    "            home_pitcher_index = indexes[19] # get index corresponding to first char of home pitcher's name\n",
    "            # retrieve the full names of the two pitchers by calling function parse_html\n",
    "            away_pitcher = parse_html(comment_str[away_pitcher_index:], \"<\")\n",
    "            home_pitcher = parse_html(comment_str[home_pitcher_index:], \"<\")\n",
    "        else:\n",
    "            sys.exit(\"Could not find starting pitchers.\")\n",
    "        # now we want to get a few different stats for each starting pitcher in the game\n",
    "        # specifically, we want innings pitched (IP), ERA (which takes into account the outcome of the game),\n",
    "        # and earned runs (ER)\n",
    "        \n",
    "        # again, the statistics of interest are embedded in a comment. We need to parse this\n",
    "        soup2 = soup1.find_all(\"div\", {\"class\" : \"section_wrapper\"})[1]\n",
    "        comment = soup2.find_all(string=lambda text:isinstance(text,Comment))[0]\n",
    "        comment_str = str(comment) # convert comment to a string\n",
    "        # as we saw before, player names are preceded with 'html\">'\n",
    "        # let's get the indexes for the two starting pitchers\n",
    "        # note: this is not redundant; this is a new comment so the name indexes have changed\n",
    "        indexes = [i+6 for i in list(find_chars(comment_str, 'html\">'))]\n",
    "        for index in indexes:\n",
    "            pitcher = parse_html(comment_str[index:], \"<\")\n",
    "            if pitcher == away_pitcher:\n",
    "                away_pitcher_index = index\n",
    "            if pitcher == home_pitcher:\n",
    "                home_pitcher_index = index\n",
    "        # the above for loop now gives us away_pitcher_index and home_pitcher index within the comment\n",
    "        # now we can call function get_pitcher_stats to retrieve the statistics of interest\n",
    "        away_pitcher_IP, home_pitcher_IP = get_pitcher_stats(comment_str, '\"IP\" > ',\n",
    "                                                             away_pitcher_index, home_pitcher_index)\n",
    "        away_pitcher_ERA, home_pitcher_ERA = get_pitcher_stats(comment_str, '\"earned_run_avg\" >',\n",
    "                                                              away_pitcher_index, home_pitcher_index)\n",
    "        away_pitcher_ER, home_pitcher_ER = get_pitcher_stats(comment_str, '\"ER\" >',\n",
    "                                                            away_pitcher_index, home_pitcher_index)\n",
    "    else:\n",
    "        # exit and throw an error if the page could not be loaded\n",
    "        sys.exit(\"Pitching page cannot be loaded.\")\n",
    "    \n",
    "    return away_pitcher, home_pitcher, away_pitcher_IP, home_pitcher_IP, away_pitcher_ERA, home_pitcher_ERA, away_pitcher_ER, home_pitcher_ER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(df, year, month, day):\n",
    "    '''\n",
    "    This function scrapes baseball-reference and retrieves many features for MLB games played on a specific date,\n",
    "    appending to a dataframe a row of features for each game\n",
    "    \n",
    "    Args:\n",
    "        df: A dataframe with columns \"away\", \"home\", \"away_score\", \"home_score\", \"year\", \"month\", \"day\"\n",
    "        year: The year of the game\n",
    "        month: The month of the game\n",
    "        day: The day of the game\n",
    "    \n",
    "    Returns:\n",
    "        df: The same dataframe that was passed in as an argument, with appended rows.\n",
    "    '''\n",
    "    # define the specific baseball reference link for MLB games played on a given date\n",
    "    url = requests.get(\"https://www.baseball-reference.com/boxes/?month=\" \\\n",
    "                       + str(month) + \"&day=\" + str(day) + \"&year=\" + str(year))\n",
    "    # ensure the page could be loaded successfully\n",
    "    if url.status_code == 200:\n",
    "        # initialize the BeautifulSoup object\n",
    "        soup = BeautifulSoup(url.content, 'html.parser')\n",
    "        # parse the HTML body to find all games played on this date\n",
    "        game_summaries = soup.find(\"body\").find(\"div\", {\"id\" : \"wrap\"}).find(\"div\", {\"id\" : \"content\"})\\\n",
    "        .find(\"div\", {\"class\" : \"game_summaries\"})\n",
    "        try:\n",
    "            # each game is uniquely defined through a \"div\" tag with \"class\"=\"game_summary_nohover\"\n",
    "            # store all the games in a list called games, which we will iterate through to grab features\n",
    "            games = game_summaries.find_all(\"div\", {\"class\" : \"game_summary nohover\"})\n",
    "            for game in games:\n",
    "                # the away team is always presented first\n",
    "                away_team_info = game.find(\"table\").find_all(\"tr\")[0]\n",
    "                # the home team is always presented second\n",
    "                home_team_info = game.find(\"table\").find_all(\"tr\")[1]\n",
    "                # note: playoff games are formatted slightly differently, but we are excluding these in\n",
    "                # the analysis, since MLB teams treat playoffs very differently than regular season games\n",
    "                away_team = away_team_info.find(\"td\").text # retrieve name of away team\n",
    "                home_team = home_team_info.find(\"td\").text # retrieve name of home team\n",
    "                # get the final scores of both teams and convert them to integers\n",
    "                away_team_score = int(away_team_info.find(\"td\", {\"class\" : \"right\"}).text)\n",
    "                home_team_score = int(home_team_info.find(\"td\", {\"class\" : \"right\"}).text)\n",
    "                # now we need to click on a new link to get specific pitcher info\n",
    "                # get the link extension\n",
    "                game_link = game.find(\"table\").find(\"tbody\").find(\"tr\").find(\"td\", {\"class\" : \"right gamelink\"}) \\\n",
    "                .find(\"a\").get(\"href\")\n",
    "                # concatenate the baseball-reference domain with game_link to get full site link\n",
    "                pitching_link = \"https://www.baseball-reference.com\" + game_link\n",
    "                \n",
    "                # pass game_link into function get_pitching_info\n",
    "                away_pitcher, home_pitcher, away_pitcher_IP, home_pitcher_IP,\\\n",
    "                away_pitcher_ERA, home_pitcher_ERA, away_pitcher_ER, home_pitcher_ER = get_pitching_info(pitching_link, year, month, day)\n",
    "                # append features as a row in dataframe\n",
    "                df = df.append({\"away\" : away_team,\n",
    "                                \"home\" : home_team,\n",
    "                                \"away_score\" : away_team_score,\n",
    "                                \"home_score\" : home_team_score,\n",
    "                                \"year\" : year,\n",
    "                                \"month\" : month,\n",
    "                                \"day\" : day,\n",
    "                                \"away_pitcher\" : away_pitcher,\n",
    "                                \"home_pitcher\" : home_pitcher,\n",
    "                                \"away_pitcher_IP\" : away_pitcher_IP,\n",
    "                                \"home_pitcher_IP\" : home_pitcher_IP,\n",
    "                                \"away_pitcher_postgame_ERA\" : away_pitcher_ERA,\n",
    "                                \"home_pitcher_postgame_ERA\" : home_pitcher_ERA,\n",
    "                                \"away_pitcher_ER\" : away_pitcher_ER,\n",
    "                                \"home_pitcher_ER\" : home_pitcher_ER}, ignore_index = True)\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        # exit and throw an error if the page could not be loaded\n",
    "        sys.exit(\"Page cannot be loaded.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the functionality is set up, we can actually retrieve the data in which we are interested by running the cell below. Note that the running time will vary slightly by machine, but expect this to take an hour or two to complete. Additionally, `BeautifulSoup` may error out once or twice during the process, because the software tries to prohibit real-time scraping updates. If you do see an error, note that `df` has been saved, so you can just updated the `start` date below and continue from where you left off unscathed.\n",
    "\n",
    "When completed, we will save the data to a csv file so that we can easily access the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Getting scores for 3/2/2013'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize an empty dataframe with the columns of interest\n",
    "df = pd.DataFrame(columns = [\"away\", \"home\", \"away_score\", \"home_score\", \"year\", \"month\", \"day\",\n",
    "                            \"away_pitcher\", \"home_pitcher\", \"away_pitcher_IP\", \"home_pitcher_IP\",\n",
    "                            \"away_pitcher_postgame_ERA\", \"home_pitcher_postgame_ERA\",\n",
    "                             \"away_pitcher_ER\", \"home_pitcher_ER\"])\n",
    "\n",
    "# enter the date range for which we want to gather game data\n",
    "dates = pd.date_range(start='3/1/2013', end='12/1/2018')\n",
    "\n",
    "# iterate through all dates and call function get_stats each time\n",
    "for i in range(len(dates)):\n",
    "    clear_output()\n",
    "    month, day, year = dates[i].month, dates[i].day, dates[i].year\n",
    "    display(\"Getting scores for \" + str(month) + \"/\" + str(day) + \"/\" + str(year))\n",
    "    if month not in [12,1,2]:\n",
    "        df = get_stats(df, year, month, day)\n",
    "\n",
    "# save file to working directory\n",
    "df.to_csv(\"df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now obtained a dataframe with 15 features for every (non-playoff) MLB game that has been played between the 2013 season and the 2018 season, inclusive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Get Earned Run Averages (ERAs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we were able to scrape starting pitchers' earned run averages (ERAs) above, it is necessary to note that these ERAs include the earned runs and innings pitched of the game of interest, which is unfortunately not what we want. We instead are interested in the ERAs of pitchers before the game begins, which was something I had a very difficult time finding throughout my data search. Despite this, earned run averages are easy enough to calculate, so below we create `pregame_ERA` variables for both `away_pitchers` and `home_pitchers`. Because we previously scraped daily post-game ERAS, this gives us a baseline that we can check our computations against.\n",
    "\n",
    "As a reference, the general formula for earned run averages is as follows:\n",
    "\n",
    "$$ERA = \\frac{\\text{total earned runs}}{\\text{total innings pitched}} * 9$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dataframe that we saved at the conclusion of part 1\n",
    "df = pd.read_csv(\"df.csv\")\n",
    "\n",
    "# initialize two new columns in dataframe that will hold pitcher cumulative ERAs\n",
    "df['home_pitcher_pregame_ERA'] = 0.0\n",
    "df['away_pitcher_pregame_ERA'] = 0.0\n",
    "\n",
    "# to compute ERAs, we need every pitcher's cumulative earned runs and cumulative innings pitched\n",
    "# we will reset these statistics at the beginning of every season\n",
    "# initialize 12 dictionaries that hold pitcher ER and IP statistics\n",
    "pitcher_ERs2013 = {}\n",
    "pitcher_IPs2013 = {}\n",
    "pitcher_ERs2014 = {}\n",
    "pitcher_IPs2014 = {}\n",
    "pitcher_ERs2015 = {}\n",
    "pitcher_IPs2015 = {}\n",
    "pitcher_ERs2016 = {}\n",
    "pitcher_IPs2016 = {}\n",
    "pitcher_ERs2017 = {}\n",
    "pitcher_IPs2017 = {}\n",
    "pitcher_ERs2018 = {}\n",
    "pitcher_IPs2018 = {}\n",
    "\n",
    "# create a list that consists of the names of the above dictionaries\n",
    "dicts = [pitcher_ERs2013, pitcher_IPs2013,\n",
    "         pitcher_ERs2014, pitcher_IPs2014,\n",
    "         pitcher_ERs2015, pitcher_IPs2015,\n",
    "         pitcher_ERs2016, pitcher_IPs2016,\n",
    "         pitcher_ERs2017, pitcher_IPs2017,\n",
    "         pitcher_ERs2018, pitcher_IPs2018]\n",
    "\n",
    "# initialize the values in all dictionaries to zero for all pitchers\n",
    "for dictionary in dicts:\n",
    "    for pitcher in df.home_pitcher.unique():\n",
    "        dictionary[pitcher] = 0.0\n",
    "    for pitcher in df.away_pitcher.unique():\n",
    "        dictionary[pitcher] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# note this cell will take a few minutes to run\n",
    "for i in range(len(df)): # for every row in df\n",
    "    year = df.year.iloc[i]\n",
    "    home_pitcher = df.home_pitcher.iloc[i]\n",
    "    away_pitcher = df.away_pitcher.iloc[i]\n",
    "    \n",
    "    # determine which dictionaries should be updated based on value for year\n",
    "    if year == 2013:\n",
    "        pitcher_ER_dict = pitcher_ERs2013\n",
    "        pitcher_IP_dict = pitcher_IPs2013\n",
    "    elif year == 2014:\n",
    "        pitcher_ER_dict = pitcher_ERs2014\n",
    "        pitcher_IP_dict = pitcher_IPs2014\n",
    "    elif year == 2015:\n",
    "        pitcher_ER_dict = pitcher_ERs2015\n",
    "        pitcher_IP_dict = pitcher_IPs2015\n",
    "    elif year == 2016:\n",
    "        pitcher_ER_dict = pitcher_ERs2016\n",
    "        pitcher_IP_dict = pitcher_IPs2016\n",
    "    elif year == 2017:\n",
    "        pitcher_ER_dict = pitcher_ERs2017\n",
    "        pitcher_IP_dict = pitcher_IPs2017\n",
    "    else:\n",
    "        pitcher_ER_dict = pitcher_ERs2018\n",
    "        pitcher_IP_dict = pitcher_IPs2018\n",
    "\n",
    "    # calculate pregame ERA for home pitcher\n",
    "    if pitcher_IP_dict[home_pitcher] == 0:\n",
    "        df.home_pitcher_pregame_ERA.iloc[i] = np.nan # if this is the pitcher's first outing, assign it a null value\n",
    "    else:\n",
    "        df.home_pitcher_pregame_ERA.iloc[i] = (pitcher_ER_dict[home_pitcher] / pitcher_IP_dict[home_pitcher]) * 9\n",
    "    # calculate pregame ERA for away pitcher\n",
    "    if pitcher_IP_dict[away_pitcher] == 0:\n",
    "        df.away_pitcher_pregame_ERA.iloc[i] = np.nan # if this is the pitcher's first outing, assign it a null value\n",
    "    else:\n",
    "        df.away_pitcher_pregame_ERA.iloc[i] = (pitcher_ER_dict[away_pitcher] / pitcher_IP_dict[away_pitcher]) * 9\n",
    "    \n",
    "    # update dictionary values with post-game ER values\n",
    "    pitcher_ER_dict[home_pitcher] = pitcher_ER_dict[home_pitcher] + df.home_pitcher_ER.iloc[i]\n",
    "    pitcher_ER_dict[away_pitcher] = pitcher_ER_dict[away_pitcher] + df.away_pitcher_ER.iloc[i]\n",
    "    \n",
    "    # update dictionary values with post-game IP values\n",
    "    # note: if IP = 6.1, this really means 6.67 (baseball statistics just uses this notation for simplicity)\n",
    "    # the extra math in the below computations adjusts for this\n",
    "    pitcher_IP_dict[home_pitcher] = pitcher_IP_dict[home_pitcher] + math.floor(df.home_pitcher_IP.iloc[i]) \\\n",
    "    + (((df.home_pitcher_IP.iloc[i] - math.floor(df.home_pitcher_IP.iloc[i])) * 10) / 3)\n",
    "    pitcher_IP_dict[away_pitcher] = pitcher_IP_dict[away_pitcher] + math.floor(df.away_pitcher_IP.iloc[i]) \\\n",
    "    + (((df.away_pitcher_IP.iloc[i] - math.floor(df.away_pitcher_IP.iloc[i])) * 10) / 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Scrape Historical Over/Under Odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We certainly need odds (i.e. the projected total runs per game) in order to build any predictive model. Many sportsbooks do not keep historical odds public (seemingly to prevent people from doing exactly what we are doing right now), but I was able to find them on a website called DonBest. The site displays the historical odds from five different sportsbooks: the odds from each source are generally very similar for any given game, but in case the odds do differ from site to site, we take the median of the five reported over/under lines.\n",
    "\n",
    "We will again use `requests` and `BeautifulSoup` to scrape the odds for each game. We will also scrape team names and team scores, so that we have fields that can create primary keys on which we will merge the odds data with game data we have collected thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_odds(df, year, month, day):\n",
    "    '''\n",
    "    This function scrapes historical over/under betting odds for MLB games played on a given date,\n",
    "    appending to a dataframe a row for each game.\n",
    "    \n",
    "    Args:\n",
    "        df: A dataframe with columns \"away\", \"home\", \"away_score\", \"home_score\", \"over_under\", \"year\", \"month\", \"day\"\n",
    "        year: The year of the game\n",
    "        month: The month of the game\n",
    "        day: The day of the game\n",
    "    \n",
    "    Returns:\n",
    "        df: The same dataframe that was passed in as an argument, with appended rows.\n",
    "    '''\n",
    "    # format the date correctly, as expected by the donbest url\n",
    "    if day < 10:\n",
    "        day = \"0\" + str(day)\n",
    "    if month < 10:\n",
    "        month = \"0\" + str(month)\n",
    "    # define the specific donbest link for MLB games that were played on a given date\n",
    "    url = requests.get(\"http://www.donbest.com/mlb/odds/\" + str(year) + str(month) + str(day) + \".html\")\n",
    "    if url.status_code == 200: # ensure the page was loaded properly\n",
    "        try:\n",
    "            # initialize BeautifulSoup object\n",
    "            soup = BeautifulSoup(url.content, 'html.parser')\n",
    "            soup1 = soup.find(\"body\").find(\"form\").find(\"div\").find(\"div\").find(\"div\", {\"id\" : \"col1\"})\n",
    "            soup2 = soup1.find(\"div\").find(\"div\").find(\"div\", {\"id\" : \"module2_2\"}).find(\"div\").find(\"div\")\n",
    "            soup3 = soup2.find(\"div\").find(\"div\", {\"id\" : \"_DivOutput\"}).find(\"div\").find(\"div\", {\"class\" : \"odds_gamesHolder\"})\n",
    "            # rows in the table are alternately called \"rows\" or \"alternateRows\"\n",
    "            rows = soup3.find(\"table\").find_all(\"tr\", {\"class\" : \"statistics_table_row\"})\n",
    "            alt_rows = soup3.find(\"table\").find_all(\"tr\", {\"class\" : \"statistics_table_alternateRow\"})\n",
    "            for i in range(len(rows)): # for every row in the table\n",
    "                try:\n",
    "                    teams = rows[i].find(\"td\", {\"class\" : \"alignLeft\"}).find(\"a\")\n",
    "                    # get the name of the away team\n",
    "                    away_team = teams.find(\"span\").text\n",
    "                    # get the name of the home team\n",
    "                    home_team = teams.find_all(\"span\")[1].text\n",
    "                    scores = rows[i].find_all(\"td\", {\"class\" : \"alignCenter\"})[1]\n",
    "                    # get away team score\n",
    "                    away_team_score = scores.find(\"div\").find(\"b\").text\n",
    "                    # get home team score\n",
    "                    home_team_score = scores.find_all(\"div\")[1].find(\"b\").text\n",
    "                    # initialize an array that holds the over/under odds from five different sportsbooks\n",
    "                    totals = []\n",
    "                    for j in range(6, 11):\n",
    "                        soup5 = float(rows[i].find_all(\"td\")[j].find(\"div\").text)\n",
    "                        if soup5 > 0 and soup5 < 25:\n",
    "                            # append the over/under line to the totals array\n",
    "                            totals.append(soup5)\n",
    "                        else:\n",
    "                            soup5 = float(rows[i].find_all(\"td\")[j].find_all(\"div\")[1].text)\n",
    "                            # append the over/under line to the totals array\n",
    "                            totals.append(soup5)\n",
    "                    # sort the totals array\n",
    "                    totals = sorted(totals)\n",
    "                    # take the median of values in the totals array\n",
    "                    over_under = statistics.median(totals)\n",
    "                    # append rows to df\n",
    "                    df = df.append({\"away\" : away_team,\n",
    "                        \"home\" : home_team,\n",
    "                        \"away_score\" : away_team_score,\n",
    "                        \"home_score\" : home_team_score,\n",
    "                        \"over_under\" : over_under,\n",
    "                        \"year\" : year,\n",
    "                        \"month\" : month,\n",
    "                        \"day\" : day}, ignore_index = True)\n",
    "                except:\n",
    "                    pass\n",
    "            # the below for loop is analogous to the one above, just now for alt_rows instead of rows\n",
    "            for i in range(len(alt_rows)):\n",
    "                try:\n",
    "                    teams = alt_rows[i].find(\"td\", {\"class\" : \"alignLeft\"}).find(\"a\")\n",
    "                    away_team = teams.find(\"span\").text\n",
    "                    home_team = teams.find_all(\"span\")[1].text\n",
    "                    scores = alt_rows[i].find_all(\"td\", {\"class\" : \"alignCenter\"})[1]\n",
    "                    away_team_score = scores.find(\"div\").find(\"b\").text\n",
    "                    home_team_score = scores.find_all(\"div\")[1].find(\"b\").text\n",
    "                    totals = []\n",
    "                    for j in range(6, 11):\n",
    "                        soup5 = float(alt_rows[i].find_all(\"td\")[j].find(\"div\").text)\n",
    "                        if soup5 > 0 and soup5 < 25:\n",
    "                            totals.append(soup5)\n",
    "                        else:\n",
    "                            soup5 = float(alt_rows[i].find_all(\"td\")[j].find_all(\"div\")[1].text)\n",
    "                            totals.append(soup5)\n",
    "                    totals = sorted(totals)\n",
    "                    over_under = statistics.median(totals)\n",
    "                    df = df.append({\"away\" : away_team,\n",
    "                        \"home\" : home_team,\n",
    "                        \"away_score\" : away_team_score,\n",
    "                        \"home_score\" : home_team_score,\n",
    "                        \"over_under\" : over_under,\n",
    "                        \"year\" : year,\n",
    "                        \"month\" : month,\n",
    "                        \"day\" : day}, ignore_index = True)\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            pass\n",
    "    # if page cannot be loaded, exit and throw an error\n",
    "    else:\n",
    "        sys.exit(\"Page cannot be loaded.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the cell below may take a bit to run, depending on the machine as well as the date range that is set (default is 3/1/2013 through 12/1/2018)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Getting odds for 5/5/2013'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize an empty dataframe with the columns of interest\n",
    "odds_df = pd.DataFrame(columns = [\"away\", \"home\", \"away_score\", \"home_score\", \"over_under\", \"year\", \"month\", \"day\"])\n",
    "\n",
    "# enter the date range for which we want to gather odds data\n",
    "dates = pd.date_range(start='3/1/2013', end='12/1/2018')\n",
    "\n",
    "# iterate through all dates and call function get_stats each time\n",
    "for i in range(len(dates)):\n",
    "    clear_output()\n",
    "    month, day, year = dates[i].month, dates[i].day, dates[i].year\n",
    "    display(\"Getting odds for \" + str(month) + \"/\" + str(day) + \"/\" + str(year))\n",
    "    if month not in [12,1,2]:\n",
    "        odds_df = get_odds(odds_df, year, month, day)\n",
    "        \n",
    "# save file to working directory\n",
    "odds_df.to_csv(\"odds_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Merge and Clean Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dataframe that we saved at the conclusion of part 1\n",
    "odds_df = pd.read_csv(\"odds_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Angels have two different syntactic names. We will update this for consistency\n",
    "df = df.replace(\"LA Angels of Anaheim\", \"Los Angeles Angels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create keys in both dataframes so that we can join them\n",
    "df[\"id\"] = df.away + df.away_score.astype(str) + df.home + df.home_score.astype(str) \\\n",
    "+ df.year.astype(str) + df.month.astype(str) + df.day.astype(str)\n",
    "odds_df[\"id\"] = odds_df.away + odds_df.away_score.astype(str) + odds_df.home + odds_df.home_score.astype(str) \\\n",
    "+ odds_df.year.astype(str) + odds_df.month.astype(str) + odds_df.day.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge df with odds_df into a new dataframe\n",
    "df = pd.merge(df, odds_df, on=\"id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0_x', 'away_x', 'home_x', 'away_score_x', 'home_score_x',\n",
       "       'year_x', 'month_x', 'day_x', 'away_pitcher', 'home_pitcher',\n",
       "       'away_pitcher_IP', 'home_pitcher_IP', 'away_pitcher_postgame_ERA',\n",
       "       'home_pitcher_postgame_ERA', 'away_pitcher_ER', 'home_pitcher_ER',\n",
       "       'home_pitcher_pregame_ERA', 'away_pitcher_pregame_ERA', 'id',\n",
       "       'Unnamed: 0_y', 'away_y', 'home_y', 'away_score_y', 'home_score_y',\n",
       "       'over_under', 'year_y', 'month_y', 'day_y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
